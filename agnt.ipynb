{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wf-eMaWvcFEt"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub pinecone-client langchain langgraph  tavily-python \"unstructured[pdf]\"\n",
        "! pip install langchain_pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "777b2HjIcQ-j"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9GU3kpCbfd3E",
        "outputId": "807d9e74-d58d-4625-fa17-1e7d209b2a04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.1.2-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (0.2.23)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (1.25.2)\n",
            "Collecting pinecone-client<5,>=3.2.2 (from langchain_pinecone)\n",
            "  Downloading pinecone_client-4.1.2-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.4/216.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (0.1.93)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (8.5.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (2024.7.4)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (2.0.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.7)\n",
            "Installing collected packages: pinecone-client, langchain_pinecone\n",
            "  Attempting uninstall: pinecone-client\n",
            "    Found existing installation: pinecone-client 5.0.0\n",
            "    Uninstalling pinecone-client-5.0.0:\n",
            "      Successfully uninstalled pinecone-client-5.0.0\n",
            "Successfully installed langchain_pinecone-0.1.2 pinecone-client-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Build Index\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# Set embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Docs to index\n",
        "\n",
        "doc_path = \"/content/anjuai.pdf\"\n",
        "# Load\n",
        "loader =PyPDFLoader(doc_path)\n",
        "documents = loader.load()\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500, chunk_overlap=200\n",
        ")\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\"\n",
        "# Add to vectorstore\n",
        "Pinecone(environment='us-east-1-aws')\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "        documents, embeddings, index_name= \"indya-cleo\")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "hDRG1iAzdQAp"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Router\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"vectorstore\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose to route it to vectorstore.\",\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to a vectorstore.\n",
        "The vectorstore contains documents about varuns data.\n",
        "Use the vectorstore for questions on these topics. Otherwise, use your own knowledge\"\"\"\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_router = route_prompt | structured_llm_router"
      ],
      "metadata": {
        "id": "D9HEMmC-GfrF"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "question = \" \"\n",
        "\n",
        "#Prompt\n",
        "# system = \"\"\"\n",
        "# You are an assistant for question-answering tasks.\n",
        "# First, use the following pieces of retrieved context to answer the question.\n",
        "# If you are unable to find the answer from the given context, then use your own knowledge.\n",
        "# Finally, if you don't know the answer, just say that you don't know.\n",
        "# {context}\n",
        "# \"\"\" # Include the context within the system message\n",
        "# prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", system),\n",
        "#         (\"human\", \"{question}\"),\n",
        "#         (\"ai\", \"answer\"), # Removed the invalid 'context' message type\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "\n",
        "prompt = hub.pull(\"indya-cleo\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.7)\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "# Assuming 'docs' contains your retrieved documents\n",
        "formatted_docs = format_docs(docs)\n",
        "generation = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
        "# print(generation)"
      ],
      "metadata": {
        "id": "uECyQOG2Glks"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph state"
      ],
      "metadata": {
        "id": "EakGx_jpHmPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[str]"
      ],
      "metadata": {
        "id": "8CwUX7881sQv"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph Flow"
      ],
      "metadata": {
        "id": "1NAhJZTfHoNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "### Edges ###\n",
        "\n",
        "\n",
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    source.datasource == \"vectorstore\"\n",
        "    print(\"---ROUTE QUESTION TO RAG---\")\n",
        "    return \"vectorstore\""
      ],
      "metadata": {
        "id": "udpU4_m52FTf"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Build Graph"
      ],
      "metadata": {
        "id": "MwwZYzauH4mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "\n",
        "# Build graph\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_question,\n",
        "    {\n",
        "        \"vectorstore\": \"retrieve\",\n",
        "    },\n",
        ")\n",
        "# workflow.add_edge(\"\"generate\")\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "oi-qNiF82J2I"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\n",
        "    \"question\": \"Tell me about bluescarf and who is its ceo\"\n",
        "}\n",
        "iteration_count = 0\n",
        "for output in app.stream(inputs):\n",
        "    iteration_count += 1\n",
        "    print(f\"Iteration: {iteration_count}\")  # Track iterations\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        pprint(value) # Print the full state at each node\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation (if reached)\n",
        "if 'generation' in value:\n",
        "    pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK9W_dZiIGwK",
        "outputId": "f5f39b86-b230-4ae2-af89-0d2c5510dc1f"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---ROUTE QUESTION---\n",
            "---ROUTE QUESTION TO RAG---\n",
            "---RETRIEVE---\n",
            "Iteration: 1\n",
            "\"Node 'retrieve':\"\n",
            "{'documents': [Document(metadata={'page': 0.0, 'source': '/content/Untitled document.pdf'}, page_content='The\\nceo\\nof\\nBluescarf.ai\\nis\\nSarfaraz\\nAhmed\\nbehan.\\nArtificial\\nIntelligence\\nServices\\nBluescarf.ai\\nbuilds\\nsolutions\\nand\\nprovides\\nresearch\\nservices\\nin\\nGenerative\\nAI,\\nAdvanced\\nAudio\\nProcessing,\\nNatural\\nLanguage\\nProcessing,\\nDeep\\nLearning,\\nFoundational\\nModels\\nand\\nother\\nAI\\ntechnologies.'),\n",
            "               Document(metadata={'page': 0.0, 'source': '/content/Untitled document.pdf'}, page_content='The\\nceo\\nof\\nBluescarf.ai\\nis\\nSarfaraz\\nAhmed\\nbehan.\\nArtificial\\nIntelligence\\nServices\\nBluescarf.ai\\nbuilds\\nsolutions\\nand\\nprovides\\nresearch\\nservices\\nin\\nGenerative\\nAI,\\nAdvanced\\nAudio\\nProcessing,\\nNatural\\nLanguage\\nProcessing,\\nDeep\\nLearning,\\nFoundational\\nModels\\nand\\nother\\nAI\\ntechnologies.'),\n",
            "               Document(metadata={'page': 0.0, 'source': '/content/Untitled document.pdf'}, page_content='The\\nceo\\nof\\nBluescarf.ai\\nis\\nSarfaraz\\nAhmed\\nbehan.\\nArtificial\\nIntelligence\\nServices\\nBluescarf.ai\\nbuilds\\nsolutions\\nand\\nprovides\\nresearch\\nservices\\nin\\nGenerative\\nAI,\\nAdvanced\\nAudio\\nProcessing,\\nNatural\\nLanguage\\nProcessing,\\nDeep\\nLearning,\\nFoundational\\nModels\\nand\\nother\\nAI\\ntechnologies.'),\n",
            "               Document(metadata={'page': 0.0, 'source': '/content/anjuai.pdf'}, page_content='The\\nceo\\nof\\nanju.ai\\nis\\nknooz\\nfatima.\\nArtificial\\nIntelligence\\nServices\\nanju.ai\\nbuilds\\nsolutions\\nonly\\nrelated\\nto\\ncomputer\\nvision.')],\n",
            " 'question': 'Tell me about bluescarf and who is its ceo'}\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "Iteration: 2\n",
            "\"Node 'generate':\"\n",
            "{'documents': [Document(metadata={'page': 0.0, 'source': '/content/Untitled document.pdf'}, page_content='The\\nceo\\nof\\nBluescarf.ai\\nis\\nSarfaraz\\nAhmed\\nbehan.\\nArtificial\\nIntelligence\\nServices\\nBluescarf.ai\\nbuilds\\nsolutions\\nand\\nprovides\\nresearch\\nservices\\nin\\nGenerative\\nAI,\\nAdvanced\\nAudio\\nProcessing,\\nNatural\\nLanguage\\nProcessing,\\nDeep\\nLearning,\\nFoundational\\nModels\\nand\\nother\\nAI\\ntechnologies.'),\n",
            "               Document(metadata={'page': 0.0, 'source': '/content/Untitled document.pdf'}, page_content='The\\nceo\\nof\\nBluescarf.ai\\nis\\nSarfaraz\\nAhmed\\nbehan.\\nArtificial\\nIntelligence\\nServices\\nBluescarf.ai\\nbuilds\\nsolutions\\nand\\nprovides\\nresearch\\nservices\\nin\\nGenerative\\nAI,\\nAdvanced\\nAudio\\nProcessing,\\nNatural\\nLanguage\\nProcessing,\\nDeep\\nLearning,\\nFoundational\\nModels\\nand\\nother\\nAI\\ntechnologies.'),\n",
            "               Document(metadata={'page': 0.0, 'source': '/content/Untitled document.pdf'}, page_content='The\\nceo\\nof\\nBluescarf.ai\\nis\\nSarfaraz\\nAhmed\\nbehan.\\nArtificial\\nIntelligence\\nServices\\nBluescarf.ai\\nbuilds\\nsolutions\\nand\\nprovides\\nresearch\\nservices\\nin\\nGenerative\\nAI,\\nAdvanced\\nAudio\\nProcessing,\\nNatural\\nLanguage\\nProcessing,\\nDeep\\nLearning,\\nFoundational\\nModels\\nand\\nother\\nAI\\ntechnologies.'),\n",
            "               Document(metadata={'page': 0.0, 'source': '/content/anjuai.pdf'}, page_content='The\\nceo\\nof\\nanju.ai\\nis\\nknooz\\nfatima.\\nArtificial\\nIntelligence\\nServices\\nanju.ai\\nbuilds\\nsolutions\\nonly\\nrelated\\nto\\ncomputer\\nvision.')],\n",
            " 'generation': 'Bluescarf.ai is a company that builds solutions and provides '\n",
            "               'research services in various Artificial Intelligence '\n",
            "               'technologies, including Generative AI, Advanced Audio '\n",
            "               'Processing, Natural Language Processing, Deep Learning, and '\n",
            "               'Foundational Models. The CEO of Bluescarf.ai is Sarfaraz Ahmed '\n",
            "               'Behan.',\n",
            " 'question': 'Tell me about bluescarf and who is its ceo'}\n",
            "'\\n---\\n'\n",
            "('Bluescarf.ai is a company that builds solutions and provides research '\n",
            " 'services in various Artificial Intelligence technologies, including '\n",
            " 'Generative AI, Advanced Audio Processing, Natural Language Processing, Deep '\n",
            " 'Learning, and Foundational Models. The CEO of Bluescarf.ai is Sarfaraz Ahmed '\n",
            " 'Behan.')\n"
          ]
        }
      ]
    }
  ]
}
