{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wf-eMaWvcFEt"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "! pip install -U langchain_community tiktoken langchain-openai langchain-cohere langchainhub pinecone-client langchain langgraph  tavily-python \"unstructured[pdf]\"\n",
        "! pip install langchain_pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-QkvkEs5F0nXvZUJnlMgcT3BlbkFJJF6SQRqOWucMf78XAUSF\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
        "### Tracing (optional)\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "777b2HjIcQ-j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9GU3kpCbfd3E",
        "outputId": "807d9e74-d58d-4625-fa17-1e7d209b2a04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.1.2-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (0.2.23)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (1.25.2)\n",
            "Collecting pinecone-client<5,>=3.2.2 (from langchain_pinecone)\n",
            "  Downloading pinecone_client-4.1.2-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.4/216.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (0.1.93)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (8.5.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (2024.7.4)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (2.0.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.7)\n",
            "Installing collected packages: pinecone-client, langchain_pinecone\n",
            "  Attempting uninstall: pinecone-client\n",
            "    Found existing installation: pinecone-client 5.0.0\n",
            "    Uninstalling pinecone-client-5.0.0:\n",
            "      Successfully uninstalled pinecone-client-5.0.0\n",
            "Successfully installed langchain_pinecone-0.1.2 pinecone-client-4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Build Index\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "\n",
        "### from langchain_cohere import CohereEmbeddings\n",
        "\n",
        "# Set embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Docs to index\n",
        "\n",
        "doc_path = \"/content/anjuai.pdf\"\n",
        "# Load\n",
        "loader =PyPDFLoader(doc_path)\n",
        "documents = loader.load()\n",
        "# docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500, chunk_overlap=200\n",
        ")\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"f2ee2cda-ce72-4788-8254-8d33f6a32558\"\n",
        "# Add to vectorstore\n",
        "Pinecone(environment='us-east-1-aws')\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "        documents, embeddings, index_name= \"indya-cleo\")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "hDRG1iAzdQAp"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Router\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"vectorstore\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose to route it to vectorstore.\",\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to a vectorstore.\n",
        "The vectorstore contains documents about varuns data.\n",
        "Use the vectorstore for questions on these topics. Otherwise, use your own knowledge\"\"\"\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_router = route_prompt | structured_llm_router\n",
        "# print(\n",
        "#     question_router.invoke(\n",
        "#         {\"question\": \"what is ai\"}\n",
        "#     )\n",
        "# )\n",
        "# print(question_router.invoke({\"question\": \"Who  is the ceo of Bluescarf.ai\"}))"
      ],
      "metadata": {
        "id": "D9HEMmC-GfrF"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Retrieval Grader\n",
        "\n",
        "\n",
        "# # Data model\n",
        "# class GradeDocuments(BaseModel):\n",
        "#     \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "#     binary_score: str = Field(\n",
        "#         description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "#     )\n",
        "\n",
        "\n",
        "# # LLM with function call\n",
        "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "# structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# # Prompt\n",
        "# system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "#     If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "#     It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "#     Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "# grade_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", system),\n",
        "#         (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# retrieval_grader = grade_prompt | structured_llm_grader\n",
        "# question = \"agent memory\"\n",
        "# # docs = retriever.get_relevant_documents(question)\n",
        "# # doc_txt = docs[1].page_content\n",
        "# # print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ],
      "metadata": {
        "id": "d1EVrIoyQn_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "question = \"who is the ceo of anju,ai\"\n",
        "# Prompt\n",
        "\n",
        "# prompt = \"\"\"\n",
        "# You are an assistant for question-answering tasks. First, use the following pieces of retrieved context to answer the question. If you are unable to find the answer from the given context, then use your own knowledge. Finally, if you don't know the answer, just say that you don't know.\n",
        "\n",
        "# Question: {question}\n",
        "\n",
        "# Context: {context}\n",
        "\n",
        "# Answer:\n",
        "\n",
        "# \"\"\"\n",
        "prompt = hub.pull(\"indya-cleo\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uECyQOG2Glks",
        "outputId": "23a14e6f-1604-401a-fabc-d62cb5a7420f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The CEO of anju.ai is Knooz Fatima.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Hallucination Grader\n",
        "\n",
        "\n",
        "# # Data model\n",
        "# class GradeHallucinations(BaseModel):\n",
        "#     \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "#     binary_score: str = Field(\n",
        "#         description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "#     )\n",
        "\n",
        "\n",
        "# # LLM with function call\n",
        "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "# structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# # Prompt\n",
        "# system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "#      Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "# hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", system),\n",
        "#         (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "# # hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ],
      "metadata": {
        "id": "vSETJceWHGtH"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Answer Grader\n",
        "\n",
        "\n",
        "# # Data model\n",
        "# class GradeAnswer(BaseModel):\n",
        "#     \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "#     binary_score: str = Field(\n",
        "#         description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "#     )\n",
        "\n",
        "\n",
        "# # LLM with function call\n",
        "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "# structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "# # Prompt\n",
        "# system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "#      Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "# answer_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", system),\n",
        "#         (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# answer_grader = answer_prompt | structured_llm_grader\n",
        "# # answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ],
      "metadata": {
        "id": "2FIe28uIHKwj"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Question Re-writer\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "# question_rewriter.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "qz685IMJHNxF"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Search Tool"
      ],
      "metadata": {
        "id": "no2lm-JdHwBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Search\n",
        "\n",
        "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "id": "7f5DT46C1Ak8"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph state"
      ],
      "metadata": {
        "id": "EakGx_jpHmPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[str]"
      ],
      "metadata": {
        "id": "8CwUX7881sQv"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph Flow"
      ],
      "metadata": {
        "id": "1NAhJZTfHoNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "# def grade_documents(state):\n",
        "#     \"\"\"\n",
        "#     Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "#     Args:\n",
        "#         state (dict): The current graph state\n",
        "\n",
        "#     Returns:\n",
        "#         state (dict): Updates documents key with only filtered relevant documents\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "#     question = state[\"question\"]\n",
        "#     documents = state[\"documents\"]\n",
        "\n",
        "#     # Score each doc\n",
        "#     filtered_docs = []\n",
        "#     for d in documents:\n",
        "#         score = retrieval_grader.invoke(\n",
        "#             {\"question\": question, \"document\": d.page_content}\n",
        "#         )\n",
        "#         grade = score.binary_score\n",
        "#         if grade == \"yes\":\n",
        "#             print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "#             filtered_docs.append(d)\n",
        "#         else:\n",
        "#             print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "#             continue\n",
        "#     return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "\n",
        "# def transform_query(state):\n",
        "#     \"\"\"\n",
        "#     Transform the query to produce a better question.\n",
        "\n",
        "#     Args:\n",
        "#         state (dict): The current graph state\n",
        "\n",
        "#     Returns:\n",
        "#         state (dict): Updates question key with a re-phrased question\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"---TRANSFORM QUERY---\")\n",
        "#     question = state[\"question\"]\n",
        "#     documents = state[\"documents\"]\n",
        "\n",
        "#     # Re-write question\n",
        "#     better_question = question_rewriter.invoke({\"question\": question})\n",
        "#     return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "# def web_search(state):\n",
        "#     \"\"\"\n",
        "#     Web search based on the re-phrased question.\n",
        "\n",
        "#     Args:\n",
        "#         state (dict): The current graph state\n",
        "\n",
        "#     Returns:\n",
        "#         state (dict): Updates documents key with appended web results\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"---WEB SEARCH---\")\n",
        "#     question = state[\"question\"]\n",
        "\n",
        "#     # Web search\n",
        "#     docs = web_search_tool.invoke({\"query\": question})\n",
        "#     web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "#     web_results = Document(page_content=web_results)\n",
        "\n",
        "#     return {\"documents\": web_results, \"question\": question}\n",
        "\n",
        "\n",
        "### Edges ###\n",
        "\n",
        "\n",
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    source.datasource == \"vectorstore\"\n",
        "    print(\"---ROUTE QUESTION TO RAG---\")\n",
        "    return \"vectorstore\"\n",
        "\n",
        "\n",
        "\n",
        "# def decide_to_generate(state):\n",
        "#     \"\"\"\n",
        "#     Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "#     Args:\n",
        "#         state (dict): The current graph state\n",
        "\n",
        "#     Returns:\n",
        "#         str: Binary decision for next node to call\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "#     state[\"question\"]\n",
        "#     filtered_documents = state[\"documents\"]\n",
        "\n",
        "#     if not filtered_documents:\n",
        "#         # All documents have been filtered check_relevance\n",
        "#         # We will re-generate a new query\n",
        "#         print(\n",
        "#             \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "#         )\n",
        "#         return \"transform_query\"\n",
        "#     else:\n",
        "#         # We have relevant documents, so generate answer\n",
        "#         print(\"---DECISION: GENERATE---\")\n",
        "#         return \"generate\"\n",
        "\n",
        "\n",
        "# def grade_generation_v_documents_and_question(state):\n",
        "#     \"\"\"\n",
        "#     Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "#     Args:\n",
        "#         state (dict): The current graph state\n",
        "\n",
        "#     Returns:\n",
        "#         str: Decision for next node to call\n",
        "#     \"\"\"\n",
        "\n",
        "#     print(\"---CHECK HALLUCINATIONS---\")\n",
        "#     question = state[\"question\"]\n",
        "#     documents = state[\"documents\"]\n",
        "#     generation = state[\"generation\"]\n",
        "\n",
        "#     score = hallucination_grader.invoke(\n",
        "#         {\"documents\": documents, \"generation\": generation}\n",
        "#     )\n",
        "#     grade = score.binary_score\n",
        "\n",
        "#     # Check hallucination\n",
        "#     if grade == \"yes\":\n",
        "#         print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "#         # Check question-answering\n",
        "#         print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "#         score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "#         grade = score.binary_score\n",
        "#         if grade == \"yes\":\n",
        "#             print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "#             return \"useful\"\n",
        "#         else:\n",
        "#             print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "#             return \"not useful\"\n",
        "#     else:\n",
        "#         pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "#         return \"not supported\""
      ],
      "metadata": {
        "id": "udpU4_m52FTf"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Build Graph"
      ],
      "metadata": {
        "id": "MwwZYzauH4mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "# workflow.add_node(\"web_search\", web_search)  # web search\n",
        "# workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "# workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "\n",
        "# Build graph\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_question,\n",
        "    {\n",
        "        \"vectorstore\": \"retrieve\",\n",
        "    },\n",
        ")\n",
        "# workflow.add_edge(\"\"generate\")\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "# workflow.add_conditional_edges(\n",
        "#     \"grade_documents\",\n",
        "#     decide_to_generate,\n",
        "#     {\n",
        "#         \"transform_query\": \"transform_query\",\n",
        "#         \"generate\": \"generate\",\n",
        "#     },\n",
        "# )\n",
        "# workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "oi-qNiF82J2I"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\n",
        "    \"question\": \"Tell me about ai in 500 words\"\n",
        "}\n",
        "iteration_count = 0\n",
        "for output in app.stream(inputs):\n",
        "    iteration_count += 1\n",
        "    print(f\"Iteration: {iteration_count}\")  # Track iterations\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        pprint(value) # Print the full state at each node\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation (if reached)\n",
        "if 'generation' in value:\n",
        "    pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK9W_dZiIGwK",
        "outputId": "992ac9a1-2f33-466b-8d7a-bbf4039444df"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---ROUTE QUESTION---\n",
            "---ROUTE QUESTION TO RAG---\n",
            "---RETRIEVE---\n",
            "Iteration: 1\n",
            "\"Node 'retrieve':\"\n",
            "{'documents': [Document(metadata={'source': '..\\\\RAG-based-Personal-AI-Assistant\\\\app\\\\data\\\\dummy.txt'}, page_content='hi'),\n",
            "               Document(metadata={'source': '..\\\\RAG-based-Personal-AI-Assistant\\\\app\\\\data\\\\dummy.txt'}, page_content='hi'),\n",
            "               Document(metadata={'source': '..\\\\RAG-based-Personal-AI-Assistant\\\\app\\\\data\\\\dummy.txt'}, page_content='hi'),\n",
            "               Document(metadata={'source': '..\\\\RAG-based-Personal-AI-Assistant\\\\app\\\\data\\\\dummy.txt'}, page_content='hi')],\n",
            " 'question': 'Tell me about ai in 500 words'}\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "Iteration: 2\n",
            "\"Node 'generate':\"\n",
            "{'documents': [Document(metadata={'source': '..\\\\RAG-based-Personal-AI-Assistant\\\\app\\\\data\\\\dummy.txt'}, page_content='hi'),\n",
            "               Document(metadata={'source': '..\\\\RAG-based-Personal-AI-Assistant\\\\app\\\\data\\\\dummy.txt'}, page_content='hi'),\n",
            "               Document(metadata={'source': '..\\\\RAG-based-Personal-AI-Assistant\\\\app\\\\data\\\\dummy.txt'}, page_content='hi'),\n",
            "               Document(metadata={'source': '..\\\\RAG-based-Personal-AI-Assistant\\\\app\\\\data\\\\dummy.txt'}, page_content='hi')],\n",
            " 'generation': 'Artificial Intelligence (AI) is a branch of computer science '\n",
            "               'that aims to create machines capable of intelligent behavior. '\n",
            "               'It involves the development of algorithms and systems that can '\n",
            "               'perform tasks typically requiring human intelligence, such as '\n",
            "               'visual perception, speech recognition, decision-making, and '\n",
            "               'language translation.\\n'\n",
            "               '\\n'\n",
            "               'AI can be broadly categorized into two types: narrow AI and '\n",
            "               'general AI. Narrow AI, also known as weak AI, is designed to '\n",
            "               'perform a specific task, such as facial recognition or '\n",
            "               'internet searches. These systems are highly specialized and '\n",
            "               'operate under a limited set of constraints. Examples of narrow '\n",
            "               'AI include virtual assistants like Siri and Alexa, '\n",
            "               'recommendation algorithms used by Netflix and Amazon, and '\n",
            "               'autonomous vehicles.\\n'\n",
            "               '\\n'\n",
            "               'General AI, or strong AI, refers to systems that possess the '\n",
            "               'ability to understand, learn, and apply knowledge in a way '\n",
            "               'that is indistinguishable from human intelligence. This type '\n",
            "               'of AI remains largely theoretical and is the subject of '\n",
            "               'ongoing research. The goal of general AI is to create machines '\n",
            "               'that can perform any intellectual task that a human can, with '\n",
            "               'the ability to reason, plan, solve problems, think abstractly, '\n",
            "               'comprehend complex ideas, learn quickly, and learn from '\n",
            "               'experience.\\n'\n",
            "               '\\n'\n",
            "               'The development of AI involves several key technologies and '\n",
            "               'methodologies:\\n'\n",
            "               '\\n'\n",
            "               '1. **Machine Learning (ML):** A subset of AI that focuses on '\n",
            "               'the development of algorithms that allow computers to learn '\n",
            "               'from and make predictions based on data. ML techniques include '\n",
            "               'supervised learning, unsupervised learning, and reinforcement '\n",
            "               'learning.\\n'\n",
            "               '\\n'\n",
            "               '2. **Deep Learning:** A subset of machine learning that uses '\n",
            "               'neural networks with many layers (hence \"deep\") to analyze '\n",
            "               'various factors of data. Deep learning has been particularly '\n",
            "               'successful in areas such as image and speech recognition.\\n'\n",
            "               '\\n'\n",
            "               '3. **Natural Language Processing (NLP):** A field of AI that '\n",
            "               'focuses on the interaction between computers and humans '\n",
            "               'through natural language. NLP enables machines to understand, '\n",
            "               'interpret, and respond to human language in a valuable way. '\n",
            "               'Applications include language translation, sentiment analysis, '\n",
            "               'and chatbots.\\n'\n",
            "               '\\n'\n",
            "               '4. **Computer Vision:** A field of AI that enables machines to '\n",
            "               'interpret and make decisions based on visual data from the '\n",
            "               'world. This technology is used in applications such as facial '\n",
            "               'recognition, object detection, and autonomous driving.\\n'\n",
            "               '\\n'\n",
            "               '5. **Robotics:** The integration of AI with robotics to create '\n",
            "               'intelligent machines capable of performing tasks in the real '\n",
            "               'world. This includes industrial robots, service robots, and '\n",
            "               'autonomous drones.\\n'\n",
            "               '\\n'\n",
            "               'AI has a wide range of applications across various '\n",
            "               'industries:\\n'\n",
            "               '\\n'\n",
            "               '- **Healthcare:** AI is used for diagnosing diseases, '\n",
            "               'personalizing treatment plans, and managing patient data. For '\n",
            "               'example, AI algorithms can analyze medical images to detect '\n",
            "               'abnormalities with high accuracy.\\n'\n",
            "               '\\n'\n",
            "               '- **Finance:** AI is employed in fraud detection, algorithmic '\n",
            "               'trading, and personalized financial advice. Machine learning '\n",
            "               'models can analyze vast amounts of financial data to identify '\n",
            "               'patterns and make predictions.\\n'\n",
            "               '\\n'\n",
            "               '- **Retail:** AI enhances customer experiences through '\n",
            "               'personalized recommendations, inventory management, and '\n",
            "               'chatbots for customer service.\\n'\n",
            "               '\\n'\n",
            "               '- **Transportation:** AI powers autonomous vehicles, optimizes '\n",
            "               'logistics and supply chain management, and improves traffic '\n",
            "               'management systems.\\n'\n",
            "               '\\n'\n",
            "               '- **Entertainment:** AI is used in content recommendation '\n",
            "               'systems, game development, and creating realistic animations '\n",
            "               'and special effects.\\n'\n",
            "               '\\n'\n",
            "               'Despite its many benefits, AI also poses ethical and societal '\n",
            "               'challenges, such as job displacement, privacy concerns, and '\n",
            "               'the potential for biased decision-making. As AI continues to '\n",
            "               'evolve, it is crucial to address these issues and ensure that '\n",
            "               'AI technologies are developed and deployed responsibly.\\n'\n",
            "               '\\n'\n",
            "               'In summary, AI is a transformative technology with the '\n",
            "               'potential to revolutionize various aspects of our lives. Its '\n",
            "               'continued development promises to bring about significant '\n",
            "               'advancements, but it also requires careful consideration of '\n",
            "               'its ethical implications.',\n",
            " 'question': 'Tell me about ai in 500 words'}\n",
            "'\\n---\\n'\n",
            "('Artificial Intelligence (AI) is a branch of computer science that aims to '\n",
            " 'create machines capable of intelligent behavior. It involves the development '\n",
            " 'of algorithms and systems that can perform tasks typically requiring human '\n",
            " 'intelligence, such as visual perception, speech recognition, '\n",
            " 'decision-making, and language translation.\\n'\n",
            " '\\n'\n",
            " 'AI can be broadly categorized into two types: narrow AI and general AI. '\n",
            " 'Narrow AI, also known as weak AI, is designed to perform a specific task, '\n",
            " 'such as facial recognition or internet searches. These systems are highly '\n",
            " 'specialized and operate under a limited set of constraints. Examples of '\n",
            " 'narrow AI include virtual assistants like Siri and Alexa, recommendation '\n",
            " 'algorithms used by Netflix and Amazon, and autonomous vehicles.\\n'\n",
            " '\\n'\n",
            " 'General AI, or strong AI, refers to systems that possess the ability to '\n",
            " 'understand, learn, and apply knowledge in a way that is indistinguishable '\n",
            " 'from human intelligence. This type of AI remains largely theoretical and is '\n",
            " 'the subject of ongoing research. The goal of general AI is to create '\n",
            " 'machines that can perform any intellectual task that a human can, with the '\n",
            " 'ability to reason, plan, solve problems, think abstractly, comprehend '\n",
            " 'complex ideas, learn quickly, and learn from experience.\\n'\n",
            " '\\n'\n",
            " 'The development of AI involves several key technologies and methodologies:\\n'\n",
            " '\\n'\n",
            " '1. **Machine Learning (ML):** A subset of AI that focuses on the development '\n",
            " 'of algorithms that allow computers to learn from and make predictions based '\n",
            " 'on data. ML techniques include supervised learning, unsupervised learning, '\n",
            " 'and reinforcement learning.\\n'\n",
            " '\\n'\n",
            " '2. **Deep Learning:** A subset of machine learning that uses neural networks '\n",
            " 'with many layers (hence \"deep\") to analyze various factors of data. Deep '\n",
            " 'learning has been particularly successful in areas such as image and speech '\n",
            " 'recognition.\\n'\n",
            " '\\n'\n",
            " '3. **Natural Language Processing (NLP):** A field of AI that focuses on the '\n",
            " 'interaction between computers and humans through natural language. NLP '\n",
            " 'enables machines to understand, interpret, and respond to human language in '\n",
            " 'a valuable way. Applications include language translation, sentiment '\n",
            " 'analysis, and chatbots.\\n'\n",
            " '\\n'\n",
            " '4. **Computer Vision:** A field of AI that enables machines to interpret and '\n",
            " 'make decisions based on visual data from the world. This technology is used '\n",
            " 'in applications such as facial recognition, object detection, and autonomous '\n",
            " 'driving.\\n'\n",
            " '\\n'\n",
            " '5. **Robotics:** The integration of AI with robotics to create intelligent '\n",
            " 'machines capable of performing tasks in the real world. This includes '\n",
            " 'industrial robots, service robots, and autonomous drones.\\n'\n",
            " '\\n'\n",
            " 'AI has a wide range of applications across various industries:\\n'\n",
            " '\\n'\n",
            " '- **Healthcare:** AI is used for diagnosing diseases, personalizing '\n",
            " 'treatment plans, and managing patient data. For example, AI algorithms can '\n",
            " 'analyze medical images to detect abnormalities with high accuracy.\\n'\n",
            " '\\n'\n",
            " '- **Finance:** AI is employed in fraud detection, algorithmic trading, and '\n",
            " 'personalized financial advice. Machine learning models can analyze vast '\n",
            " 'amounts of financial data to identify patterns and make predictions.\\n'\n",
            " '\\n'\n",
            " '- **Retail:** AI enhances customer experiences through personalized '\n",
            " 'recommendations, inventory management, and chatbots for customer service.\\n'\n",
            " '\\n'\n",
            " '- **Transportation:** AI powers autonomous vehicles, optimizes logistics and '\n",
            " 'supply chain management, and improves traffic management systems.\\n'\n",
            " '\\n'\n",
            " '- **Entertainment:** AI is used in content recommendation systems, game '\n",
            " 'development, and creating realistic animations and special effects.\\n'\n",
            " '\\n'\n",
            " 'Despite its many benefits, AI also poses ethical and societal challenges, '\n",
            " 'such as job displacement, privacy concerns, and the potential for biased '\n",
            " 'decision-making. As AI continues to evolve, it is crucial to address these '\n",
            " 'issues and ensure that AI technologies are developed and deployed '\n",
            " 'responsibly.\\n'\n",
            " '\\n'\n",
            " 'In summary, AI is a transformative technology with the potential to '\n",
            " 'revolutionize various aspects of our lives. Its continued development '\n",
            " 'promises to bring about significant advancements, but it also requires '\n",
            " 'careful consideration of its ethical implications.')\n"
          ]
        }
      ]
    }
  ]
}